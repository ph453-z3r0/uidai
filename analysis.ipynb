{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# UIDAI Aadhaar Data Analysis\n",
                "\n",
                "This notebook analyzes Aadhaar enrolment, demographic, and biometric update data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Prophet if not already installed\n",
                "!pip install prophet\n",
                "\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.ensemble import IsolationForest\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from prophet import Prophet\n",
                "import numpy as np\n",
                "\n",
                "# Set plot style\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define file paths\n",
                "enrolment_path = 'api_data_aadhar_enrolment/api_data_aadhar_enrolment_0_500000.csv'\n",
                "demographic_path = 'api_data_aadhar_demographic/api_data_aadhar_demographic_0_500000.csv'\n",
                "biometric_path = 'api_data_aadhar_biometric/api_data_aadhar_biometric_0_500000.csv'\n",
                "\n",
                "# Load CSVs\n",
                "df_enrolment = pd.read_csv(enrolment_path)\n",
                "df_demographic = pd.read_csv(demographic_path)\n",
                "df_biometric = pd.read_csv(biometric_path)\n",
                "\n",
                "print(\"Data loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preprocessing & Basic Info"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert date columns to datetime\n",
                "df_enrolment['date'] = pd.to_datetime(df_enrolment['date'], format='%Y%m%d', errors='coerce')\n",
                "df_demographic['date'] = pd.to_datetime(df_demographic['date'], format='%Y%m%d', errors='coerce')\n",
                "df_biometric['date'] = pd.to_datetime(df_biometric['date'], format='%Y%m%d', errors='coerce')\n",
                "\n",
                "# Display basic info\n",
                "print(\"--- Enrolment Data ---\")\n",
                "print(f\"Shape: {df_enrolment.shape}\")\n",
                "print(df_enrolment.info())\n",
                "print(\"\\nMissing Values:\\n\", df_enrolment.isnull().sum())\n",
                "\n",
                "print(\"\\n--- Demographic Data ---\")\n",
                "print(f\"Shape: {df_demographic.shape}\")\n",
                "print(df_demographic.info())\n",
                "print(\"\\nMissing Values:\\n\", df_demographic.isnull().sum())\n",
                "\n",
                "print(\"\\n--- Biometric Data ---\")\n",
                "print(f\"Shape: {df_biometric.shape}\")\n",
                "print(df_biometric.info())\n",
                "print(\"\\nMissing Values:\\n\", df_biometric.isnull().sum())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Aggregation to Monthly Totals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add month column for aggregation\n",
                "df_enrolment['month'] = df_enrolment['date'].dt.to_period('M')\n",
                "df_demographic['month'] = df_demographic['date'].dt.to_period('M')\n",
                "df_biometric['month'] = df_biometric['date'].dt.to_period('M')\n",
                "\n",
                "# Aggregate Enrolment\n",
                "enrolment_agg = df_enrolment.groupby(['state', 'district', 'pincode', 'month'])[['age_0_5', 'age_5_17', 'age_18_greater']].sum().reset_index()\n",
                "enrolment_agg['total_enrolment'] = enrolment_agg['age_0_5'] + enrolment_agg['age_5_17'] + enrolment_agg['age_18_greater']\n",
                "\n",
                "# Aggregate Demographic\n",
                "demographic_agg = df_demographic.groupby(['state', 'district', 'pincode', 'month'])[['demo_age_5_17', 'demo_age_17_']].sum().reset_index()\n",
                "demographic_agg['total_demographic'] = demographic_agg['demo_age_5_17'] + demographic_agg['demo_age_17_']\n",
                "\n",
                "# Aggregate Biometric\n",
                "biometric_agg = df_biometric.groupby(['state', 'district', 'pincode', 'month'])[['bio_age_5_17', 'bio_age_17_']].sum().reset_index()\n",
                "biometric_agg['total_biometric'] = biometric_agg['bio_age_5_17'] + biometric_agg['bio_age_17_']\n",
                "\n",
                "print(\"Aggregation complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Sample Rows (Uttar Pradesh & Bihar)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Sample: Uttar Pradesh (Enrolment) ---\")\n",
                "display(df_enrolment[df_enrolment['state'] == 'Uttar Pradesh'].head())\n",
                "\n",
                "print(\"\\n--- Sample: Bihar (Enrolment) ---\")\n",
                "display(df_enrolment[df_enrolment['state'] == 'Bihar'].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate by month for the whole country\n",
                "monthly_enrolment = df_enrolment.groupby('month')[['age_0_5', 'age_5_17', 'age_18_greater']].sum().sum(axis=1)\n",
                "monthly_demographic = df_demographic.groupby('month')[['demo_age_5_17', 'demo_age_17_']].sum().sum(axis=1)\n",
                "monthly_biometric = df_biometric.groupby('month')[['bio_age_5_17', 'bio_age_17_']].sum().sum(axis=1)\n",
                "\n",
                "# Create a combined dataframe for plotting\n",
                "plot_df = pd.DataFrame({\n",
                "    'Enrolment': monthly_enrolment,\n",
                "    'Demographic Update': monthly_demographic,\n",
                "    'Biometric Update': monthly_biometric\n",
                "})\n",
                "\n",
                "# Plot\n",
                "plot_df.plot(kind='line', marker='o')\n",
                "plt.title('Monthly Trends: Enrolment vs Updates')\n",
                "plt.xlabel('Month')\n",
                "plt.ylabel('Count')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. State-wise Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge aggregated dataframes for a complete view (outer join to keep all states)\n",
                "# Note: This is a simplified merge on state for summary purposes\n",
                "\n",
                "state_enrolment = df_enrolment.groupby('state')[['age_0_5', 'age_5_17', 'age_18_greater']].sum()\n",
                "state_enrolment['Total Enrolment'] = state_enrolment.sum(axis=1)\n",
                "\n",
                "state_demographic = df_demographic.groupby('state')[['demo_age_5_17', 'demo_age_17_']].sum()\n",
                "state_demographic['Total Demographic'] = state_demographic.sum(axis=1)\n",
                "\n",
                "state_biometric = df_biometric.groupby('state')[['bio_age_5_17', 'bio_age_17_']].sum()\n",
                "state_biometric['Total Biometric'] = state_biometric.sum(axis=1)\n",
                "\n",
                "# Combine\n",
                "state_summary = pd.concat([state_enrolment['Total Enrolment'], \n",
                "                           state_demographic['Total Demographic'], \n",
                "                           state_biometric['Total Biometric']], axis=1)\n",
                "\n",
                "# Sort by Total Enrolment and show top 10\n",
                "print(\"--- Top 10 States by Enrolment ---\")\n",
                "display(state_summary.sort_values('Total Enrolment', ascending=False).head(10))\n",
                "\n",
                "# Plot State-wise comparison for top 10\n",
                "state_summary.sort_values('Total Enrolment', ascending=False).head(10).plot(kind='bar', stacked=False)\n",
                "plt.title('Top 10 States: Enrolment vs Updates')\n",
                "plt.xlabel('State')\n",
                "plt.ylabel('Count')\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. MBU Risk Command Centre"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Data Preparation\n",
                "# Merge Enrolment and Biometric data on key columns\n",
                "mbu_data = pd.merge(enrolment_agg, biometric_agg, \n",
                "                    on=['state', 'district', 'pincode', 'month'], \n",
                "                    how='outer').fillna(0)\n",
                "\n",
                "# Calculate E_5_17 and B_5_17\n",
                "mbu_data['E_5_17'] = mbu_data['age_5_17']\n",
                "mbu_data['B_5_17'] = mbu_data['bio_age_5_17']\n",
                "\n",
                "# 2. Risk Calculation\n",
                "# R_bio_5_17 = B_5_17 / (E_5_17 + 1)\n",
                "mbu_data['R_bio_5_17'] = mbu_data['B_5_17'] / (mbu_data['E_5_17'] + 1)\n",
                "\n",
                "# Normalize R_bio_5_17 within each state to 0-1\n",
                "def normalize_group(group):\n",
                "    min_val = group.min()\n",
                "    max_val = group.max()\n",
                "    if max_val - min_val == 0:\n",
                "        return pd.Series([0] * len(group), index=group.index)\n",
                "    return (group - min_val) / (max_val - min_val)\n",
                "\n",
                "mbu_data['normalized_R_bio_5_17'] = mbu_data.groupby('state')['R_bio_5_17'].transform(normalize_group)\n",
                "\n",
                "# MBU_Risk = 1 - normalized_R_bio_5_17\n",
                "mbu_data['MBU_Risk'] = 1 - mbu_data['normalized_R_bio_5_17']\n",
                "\n",
                "# Add columns: mbu_risk_score, mbu_risk_category, children_at_risk\n",
                "mbu_data['mbu_risk_score'] = mbu_data['MBU_Risk']\n",
                "\n",
                "def get_risk_category(score):\n",
                "    if score < 0.3:\n",
                "        return 'Green'\n",
                "    elif score <= 0.6:\n",
                "        return 'Yellow'\n",
                "    else:\n",
                "        return 'Red'\n",
                "\n",
                "mbu_data['mbu_risk_category'] = mbu_data['mbu_risk_score'].apply(get_risk_category)\n",
                "mbu_data['children_at_risk'] = mbu_data['E_5_17'] * mbu_data['mbu_risk_score']\n",
                "\n",
                "print(\"MBU Risk Calculation Complete.\")\n",
                "display(mbu_data.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Visualization & Reporting\n",
                "\n",
                "# 1. Top 10 highest risk pincodes\n",
                "print(\"--- Top 10 Highest Risk Pincodes ---\")\n",
                "top_10_risk = mbu_data.sort_values('MBU_Risk', ascending=False).head(10)\n",
                "display(top_10_risk[['state', 'district', 'pincode', 'month', 'MBU_Risk', 'mbu_risk_category', 'children_at_risk']])\n",
                "\n",
                "# 2. Heatmap for Uttar Pradesh by district\n",
                "print(\"\\n--- Heatmap: Uttar Pradesh MBU Risk by District ---\")\n",
                "up_data = mbu_data[mbu_data['state'] == 'Uttar Pradesh']\n",
                "if not up_data.empty:\n",
                "    heatmap_data = up_data.pivot_table(index='district', columns='month', values='MBU_Risk', aggfunc='mean')\n",
                "    plt.figure(figsize=(12, 10))\n",
                "    sns.heatmap(heatmap_data, cmap='RdYlGn_r', annot=False)\n",
                "    plt.title('Uttar Pradesh: Average MBU Risk by District and Month')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No data found for Uttar Pradesh.\")\n",
                "\n",
                "# 3. Time trend for top 3 risky districts\n",
                "print(\"\\n--- Time Trend: Top 3 Risky Districts ---\")\n",
                "# Find top 3 districts by average risk across all time\n",
                "district_risk = mbu_data.groupby('district')['MBU_Risk'].mean().sort_values(ascending=False)\n",
                "top_3_districts = district_risk.head(3).index.tolist()\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "for district in top_3_districts:\n",
                "    district_data = mbu_data[mbu_data['district'] == district]\n",
                "    # Group by month to handle multiple pincodes in a district\n",
                "    monthly_risk = district_data.groupby('month')['MBU_Risk'].mean()\n",
                "    monthly_risk.plot(label=district, marker='o')\n",
                "\n",
                "plt.title(f'MBU Risk Trend for Top 3 Risky Districts: {top_3_districts}')\n",
                "plt.xlabel('Month')\n",
                "plt.ylabel('Average MBU Risk')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "\n",
                "# 4. State-wise risk distribution boxplot\n",
                "print(\"\\n--- State-wise Risk Distribution ---\")\n",
                "plt.figure(figsize=(15, 8))\n",
                "sns.boxplot(x='state', y='MBU_Risk', data=mbu_data)\n",
                "plt.xticks(rotation=90)\n",
                "plt.title('Distribution of MBU Risk Scores by State')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Results\n",
                "mbu_data.to_csv('mbu_risk_data.csv', index=False)\n",
                "print(\"Results saved to 'mbu_risk_data.csv'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Fraud Radar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Feature Engineering\n",
                "\n",
                "# Merge all three aggregations\n",
                "fraud_data = pd.merge(enrolment_agg, demographic_agg, \n",
                "                      on=['state', 'district', 'pincode', 'month'], \n",
                "                      how='outer').fillna(0)\n",
                "fraud_data = pd.merge(fraud_data, biometric_agg, \n",
                "                      on=['state', 'district', 'pincode', 'month'], \n",
                "                      how='outer').fillna(0)\n",
                "\n",
                "# Calculate Totals\n",
                "fraud_data['E_total'] = fraud_data['total_enrolment']\n",
                "fraud_data['D_total'] = fraud_data['total_demographic']\n",
                "fraud_data['B_total'] = fraud_data['total_biometric']\n",
                "\n",
                "# Calculate Ratios\n",
                "fraud_data['update_to_enrol'] = (fraud_data['D_total'] + fraud_data['B_total']) / (fraud_data['E_total'] + 1)\n",
                "fraud_data['bio_to_demo'] = fraud_data['B_total'] / (fraud_data['D_total'] + 1)\n",
                "\n",
                "# Calculate Volume Z-Score (grouped by district)\n",
                "def calculate_zscore(group):\n",
                "    if group.std() == 0:\n",
                "        return pd.Series([0] * len(group), index=group.index)\n",
                "    return (group - group.mean()) / group.std()\n",
                "\n",
                "fraud_data['volume_zscore'] = fraud_data.groupby('district')['E_total'].transform(calculate_zscore)\n",
                "\n",
                "print(\"Feature Engineering Complete.\")\n",
                "display(fraud_data.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Model Training\n",
                "\n",
                "features = ['E_total', 'D_total', 'B_total', 'update_to_enrol', 'bio_to_demo']\n",
                "X = fraud_data[features]\n",
                "\n",
                "# Handle infinite values if any (though +1 denominator should prevent it, good practice)\n",
                "X = X.replace([float('inf'), -float('inf')], 0)\n",
                "\n",
                "# KMeans Clustering\n",
                "kmeans = KMeans(n_clusters=3, random_state=42)\n",
                "fraud_data['cluster_label'] = kmeans.fit_predict(X)\n",
                "\n",
                "# Isolation Forest for Anomaly Detection\n",
                "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
                "iso_forest.fit(X)\n",
                "\n",
                "# Calculate Anomaly Score\n",
                "# decision_function returns negative for anomalies, positive for normal.\n",
                "# We want a score where higher is more anomalous.\n",
                "# One way: 0.5 - decision_function (roughly)\n",
                "# Or just use the decision function and invert/scale it.\n",
                "scores = iso_forest.decision_function(X)\n",
                "fraud_data['raw_anomaly_score'] = scores\n",
                "\n",
                "# Normalize score to 0-1 range where 1 is most anomalous\n",
                "# The lower the score, the more anomalous. So we invert it.\n",
                "min_score = scores.min()\n",
                "max_score = scores.max()\n",
                "fraud_data['anomaly_score'] = 1 - ((scores - min_score) / (max_score - min_score))\n",
                "\n",
                "print(\"Model Training Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Rule-based Flagging\n",
                "\n",
                "fraud_data['HIGH_VOLUME'] = fraud_data['volume_zscore'] > 3\n",
                "fraud_data['MIX_ANOMALY'] = (fraud_data['bio_to_demo'] > 0.9) | (fraud_data['bio_to_demo'] < 0.1)\n",
                "fraud_data['BULK_EVENT'] = fraud_data['update_to_enrol'] > 2.0\n",
                "\n",
                "# Flag Top 1% as Hotspot for Audit\n",
                "threshold = fraud_data['anomaly_score'].quantile(0.99)\n",
                "fraud_data['HOTSPOT_AUDIT'] = fraud_data['anomaly_score'] > threshold\n",
                "\n",
                "print(f\"Hotspot Threshold (Top 1%): {threshold:.4f}\")\n",
                "print(f\"Number of Hotspots: {fraud_data['HOTSPOT_AUDIT'].sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Visualization & Reporting\n",
                "\n",
                "# 1. Anomaly Score Distribution\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.histplot(fraud_data['anomaly_score'], bins=50, kde=True)\n",
                "plt.title('Distribution of Anomaly Scores')\n",
                "plt.xlabel('Anomaly Score (Higher = More Anomalous)')\n",
                "plt.show()\n",
                "\n",
                "# 2. Top 20 Anomalous Pincodes\n",
                "print(\"--- Top 20 Anomalous Pincodes ---\")\n",
                "top_20_anomalies = fraud_data.sort_values('anomaly_score', ascending=False).head(20)\n",
                "display(top_20_anomalies[['state', 'district', 'pincode', 'month', 'anomaly_score', \n",
                "                          'HIGH_VOLUME', 'MIX_ANOMALY', 'BULK_EVENT', 'HOTSPOT_AUDIT']])\n",
                "\n",
                "# 3. Scatter: update_to_enrol vs bio_to_demo (color by anomaly_score)\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.scatterplot(data=fraud_data, x='update_to_enrol', y='bio_to_demo', \n",
                "                hue='anomaly_score', palette='viridis', size='E_total', sizes=(20, 200))\n",
                "plt.title('Fraud Radar: Update Ratio vs Bio-Demo Mix (Colored by Anomaly Score)')\n",
                "plt.xlabel('Update to Enrolment Ratio')\n",
                "plt.ylabel('Biometric to Demographic Ratio')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Results\n",
                "fraud_data.to_csv('fraud_radar_data.csv', index=False)\n",
                "print(\"Results saved to 'fraud_radar_data.csv'.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Migration Planner"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Identify Top 20 Urban Pincodes\n",
                "\n",
                "# Calculate average D_total per pincode\n",
                "pincode_avg_updates = demographic_agg.groupby('pincode')['total_demographic'].mean().sort_values(ascending=False)\n",
                "top_20_pincodes = pincode_avg_updates.head(20).index.tolist()\n",
                "\n",
                "print(f\"Top 20 Urban Pincodes identified: {top_20_pincodes}\")\n",
                "\n",
                "# Filter data for these pincodes\n",
                "migration_data = demographic_agg[demographic_agg['pincode'].isin(top_20_pincodes)].copy()\n",
                "migration_data['ds'] = migration_data['month'].dt.to_timestamp()\n",
                "migration_data = migration_data.rename(columns={'total_demographic': 'y'})\n",
                "\n",
                "# 2. Forecasting Loop\n",
                "\n",
                "forecast_results = []\n",
                "\n",
                "plt.figure(figsize=(15, 10))\n",
                "plot_count = 0\n",
                "\n",
                "for pincode in top_20_pincodes:\n",
                "    # Prepare data for this pincode\n",
                "    df_prophet = migration_data[migration_data['pincode'] == pincode][['ds', 'y']].sort_values('ds')\n",
                "    \n",
                "    if len(df_prophet) < 2:\n",
                "        continue # Not enough data points\n",
                "        \n",
                "    # Fit Prophet Model\n",
                "    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
                "    m.fit(df_prophet)\n",
                "    \n",
                "    # Forecast 6 months ahead\n",
                "    future = m.make_future_dataframe(periods=6, freq='M')\n",
                "    forecast = m.predict(future)\n",
                "    \n",
                "    # 3. Capacity Planning\n",
                "    historical_avg = df_prophet['y'].mean()\n",
                "    # Get only the future forecast part (last 6 rows)\n",
                "    future_forecast = forecast.tail(6)\n",
                "    forecast_avg = future_forecast['yhat'].mean()\n",
                "    forecast_monthly = forecast_avg # Using average of next 6 months as the monthly demand\n",
                "    \n",
                "    # Surge Score\n",
                "    surge_score = (forecast_avg - historical_avg) / historical_avg if historical_avg > 0 else 0\n",
                "    \n",
                "    # M/M/1 Queue Model\n",
                "    # Î» = forecast_monthly / (22 days * 8 hours)\n",
                "    arrival_rate_lambda = forecast_monthly / (22 * 8)\n",
                "    service_rate_mu = 2 # updates/hour\n",
                "    \n",
                "    # Recommended machines\n",
                "    # Simple capacity: machines >= lambda / mu\n",
                "    recommended_machines = np.ceil(arrival_rate_lambda / service_rate_mu)\n",
                "    \n",
                "    forecast_results.append({\n",
                "        'pincode': pincode,\n",
                "        'historical_avg': historical_avg,\n",
                "        'forecast_demand': forecast_monthly,\n",
                "        'surge_pct': surge_score * 100,\n",
                "        'rec_machines': int(recommended_machines)\n",
                "    })\n",
                "    \n",
                "    # Plot top 5\n",
                "    if plot_count < 5:\n",
                "        plt.subplot(3, 2, plot_count + 1)\n",
                "        plt.plot(df_prophet['ds'], df_prophet['y'], label='Actual')\n",
                "        plt.plot(forecast['ds'], forecast['yhat'], label='Forecast', linestyle='--')\n",
                "        plt.title(f'Pincode: {pincode}')\n",
                "        plt.legend()\n",
                "        plot_count += 1\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "forecast_df = pd.DataFrame(forecast_results)\n",
                "print(\"Forecasting Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Visualization & Reporting\n",
                "\n",
                "# 1. Summary Table\n",
                "print(\"--- Migration Forecast Summary ---\")\n",
                "display(forecast_df.sort_values('surge_pct', ascending=False))\n",
                "\n",
                "# 2. Heatmap: Pincodes by Forecasted Surge\n",
                "plt.figure(figsize=(12, 8))\n",
                "# Create a dummy matrix for heatmap visualization of 1D data\n",
                "surge_data = forecast_df.set_index('pincode')[['surge_pct']].sort_values('surge_pct', ascending=False)\n",
                "sns.heatmap(surge_data, cmap='coolwarm', annot=True, fmt='.1f')\n",
                "plt.title('Forecasted Demand Surge (%) by Pincode')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Results\n",
                "forecast_df.to_csv('migration_forecast_data.csv', index=False)\n",
                "print(\"Results saved to 'migration_forecast_data.csv'.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}